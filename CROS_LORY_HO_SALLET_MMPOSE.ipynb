{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b0e41d-1e3e-4628-aa0c-d39aeef0eca3",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **Analyse de la Danse de l'Égyptien**\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c057fd-0b8d-4e67-808d-475445934e70",
   "metadata": {},
   "source": [
    "### I. INSTALLATION ET UTILISATION DE LA LIBRAIRIE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ceb28a-cb82-48d2-9fae-e42c808425c0",
   "metadata": {},
   "source": [
    "Afin de garantir la fiabilité de nos futures analyses de trajectoires, nous mettons en place un test avec l'outil MMPoseInferencer. L'objectif est de vérifier la fiabilité de la lecture et la visualisation des données.\n",
    "\n",
    "Nous ciblons ici l'extraction des articulations standards (épaules, coudes, poignets) qui seront les variables importante pour la suite de notre projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2be9dd-0074-4642-887f-e02f74d82790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmengine\n",
    "import mmcv\n",
    "import mmdet\n",
    "import mmpose\n",
    "from mmpose.apis import MMPoseInferencer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.animation as animation\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb98d66-0193-454a-995b-58be65ebe882",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inferencer = MMPoseInferencer('human', device='cpu') \n",
    "\n",
    "image = 'demo.jpg'\n",
    "result_generator = inferencer(image, vis=True, out_dir='vis_results')\n",
    "result = next(result_generator)\n",
    "\n",
    "vis_img_path = os.path.join('vis_results', 'visualizations', os.path.basename(image))\n",
    "vis_img = mmcv.imread(vis_img_path)\n",
    "\n",
    "plt.imshow(mmcv.bgr2rgb(vis_img))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026b5d8-70f4-4606-82a6-032bc3e08be6",
   "metadata": {},
   "source": [
    "On remarque bien ici les articulations du corps avec les yeux, les oreilles et le nez. Nous pouvons donc passer à la suite du projet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd70fa-c97b-4a2f-a32d-388fed7dc8ab",
   "metadata": {},
   "source": [
    "### II. PRÉSENTATION DES DONNÉES UTILISÉES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7913c-c7c3-4cbc-b1ea-4a52e675fd05",
   "metadata": {},
   "source": [
    "Nous avons choisi de travailler sur une vidéo du jeu Just Dance. Nous avons donc deux vidéos à analyser :\n",
    "- La vidéo originale de la danse \n",
    "- Une vidéo de nous qui reproduisons cette même danse\n",
    "\n",
    "Notre but est de comparer nos mouvements par rapport à la vidéo originale pour savoir qui de nous reproduit le mieux la danse.\n",
    "Nous avons choisi un mouvement assez répétitif afin de pouvoir en tirer une prédiction sur les prochains mouvements.\n",
    "\n",
    "Nous n'avons pas encore filmé la vidéo de nous qui dansons. Pour l'instant nous allons baser nos analyses sur la vidéo originale et nous rajouterons la semaine prochaine notre vidéo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3b55e-b841-48a1-a537-6e410492d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous avons pris un extrait raw de just dance afin d'éviter le bruit sur la vidéo\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"original_30fps.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004f310-9563-4977-aa9e-ae6b4652e2b7",
   "metadata": {},
   "source": [
    "### PREMIERS RÉSULTATS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ad4a4-8427-47c5-ba8c-40310ccfa03d",
   "metadata": {},
   "source": [
    "Utilisons MMPoseInferencer afin d'extraire les keypoints de nos vidéos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f371caf-ec6a-48ec-847e-14a12cc0b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = 'original_30fps.mp4'\n",
    "\n",
    "result_generator = inferencer(original, vis=True, out_dir='vis_results')\n",
    "\n",
    "for frame_result in result_generator:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d98bd-1503-4111-8ab9-a2283d1812aa",
   "metadata": {},
   "source": [
    "L'outil nous génère deux fichiers : \n",
    "- Un fichier vidéo mp4 qui reprend la vidéo de base en ajoutant les keypoints directement sur l'image\n",
    "- Un fichier json qui contient toutes les coordonnées de chaque keypoints image par image et personne par personne\n",
    "\n",
    "C'est sur ce dernier fichier json que nous allons pouvoir étudier les trajectoires et mouvements de nos personnages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9033f-61ae-46e6-9a29-6ab956d32fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\" \n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"vis_results/visualizations/original.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116daa40-bfa6-4d68-823f-405ec45d2b04",
   "metadata": {},
   "source": [
    "BATT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0138d11-7567-4c6b-aa21-931e2be8a439",
   "metadata": {},
   "source": [
    "### Battle de danse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297075e-d383-458b-badf-46a6d3d19600",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"slayeuse.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b357a5-8157-4c7d-b7c9-afc7ca9701e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = 'slayeuse.mp4'\n",
    "result_generator = inferencer(original, vis=True, out_dir='vis_results')\n",
    "for frame_result in result_generator:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b247cd2-4899-48b2-8381-ef35b0fdde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def normaliser_squelette(kp):\n",
    "    # Centre sur le bassin et met à l'échelle\n",
    "    hip_center = (kp[11] + kp[12]) / 2\n",
    "    kp_centered = kp - hip_center\n",
    "    shoulder_center = (kp_centered[5] + kp_centered[6]) / 2\n",
    "    torso_size = np.linalg.norm(shoulder_center)\n",
    "    if torso_size == 0: return kp_centered\n",
    "    return kp_centered / torso_size\n",
    "\n",
    "def charger_reference(fichier_ref):\n",
    "    # Charge le fichier original\n",
    "    with open(fichier_ref, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    trajectoire = []\n",
    "    for frame in data:\n",
    "        if len(frame['instances']) > 0:\n",
    "            kps = np.array(frame['instances'][0]['keypoints'])\n",
    "            if kps.shape[1] == 3: kps = kps[:, :2]\n",
    "            trajectoire.append(normaliser_squelette(kps))\n",
    "    return np.array(trajectoire)\n",
    "\n",
    "def extraire_groupe(fichier_json_groupe, nb_personnes=4):\n",
    "    # Charge le fichier du groupeet trie de gauche à droite\n",
    "    with open(fichier_json_groupe, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    trajectoires = [[] for _ in range(nb_personnes)]\n",
    "    \n",
    "    for frame in data:\n",
    "        instances = frame.get('instances', [])\n",
    "        if len(instances) < nb_personnes: continue \n",
    "        \n",
    "        # Récupération et Tri (Gauche -> Droite)\n",
    "        personnes_brutes = []\n",
    "        for inst in instances:\n",
    "            kps = np.array(inst['keypoints'])\n",
    "            if kps.shape[1] == 3: kps = kps[:, :2]\n",
    "            personnes_brutes.append(kps)\n",
    "        \n",
    "        personnes_triees = sorted(personnes_brutes, key=lambda p: np.mean(p[:, 0]))\n",
    "        personnes_triees = personnes_triees[:nb_personnes]\n",
    "        \n",
    "        for i, kps in enumerate(personnes_triees):\n",
    "            trajectoires[i].append(normaliser_squelette(kps))\n",
    "            \n",
    "    return [np.array(t) for t in trajectoires]\n",
    "\n",
    "def calculer_score_similitude(ref, student):\n",
    "    n = min(len(ref), len(student))\n",
    "    if n == 0: return 0.0\n",
    "    \n",
    "    ref = ref[:n]\n",
    "    stud = student[:n]\n",
    "    \n",
    "    diff = ref - stud\n",
    "    erreur_moyenne = np.mean(np.linalg.norm(diff, axis=(1,2)))\n",
    "    facteur_severite = 0.3 \n",
    "    \n",
    "    score_final = 100 * np.exp(-facteur_severite * erreur_moyenne)\n",
    "    return score_final\n",
    "    \n",
    "data_ref = charger_reference('vis_results/predictions/original.json')\n",
    "danseuses = extraire_groupe('vis_results/predictions/slayeuse.json', nb_personnes=4)\n",
    "\n",
    "noms = [\"Gauche\", \"Milieu-Gauche\", \"Milieu-Droite\", \"Droite\"]\n",
    "resultats = {}\n",
    "\n",
    "for i in range(4):\n",
    "    if len(danseuses[i]) > 0:\n",
    "        score = calculer_score_similitude(data_ref, danseuses[i])\n",
    "        resultats[noms[i]] = score\n",
    "    else:\n",
    "        print(f\" Pas assez de données détectées pour {noms[i]}\")\n",
    "\n",
    "print(\"\\n===  RÉSULTATS BATTLE DE DANSE  ===\")\n",
    "# reverse=True pour avoir le 1er (le plus haut score) en haut\n",
    "for nom, sc in sorted(resultats.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{nom} : {sc:.1f} % de réussite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ac2b9-f17b-4ca5-a083-e95acd396ddc",
   "metadata": {},
   "source": [
    "### III. ANALYSE DES TRAJECTOIRES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be814797-096d-4687-93ed-9fd7953524a8",
   "metadata": {},
   "source": [
    "Nous allons utiliser le fichier json pour extraire la positions des poignets du danseur frame par frame. Cependant, les coordonnées brutes extraites par MMPose sont exprimées en pixels. Nous ne pouvons donc pas travailler directement avec ces valeurs car :\n",
    "- Un danseur proche de l'objectif aura des mouvements qui paraissent \"plus grands\" qu'un danseur au second plan.\n",
    "- La taille des membres varie d'un individu à l'autre.\n",
    "\n",
    "Pour l'instant cela ne pose pas de problème car nous avons qu'une seule vidéo à analyser mais quand nous ajouterons l'analyse de notre danse, nous aurons besoin de données normalisées.\n",
    "\n",
    "Nous déplaçons donc l'origine du repère au milieu de la ligne des épaules. Ainsi, la position du poignet est mesurée relativement au buste et non au cadre de l'image.\n",
    "\n",
    "La formule appliquée pour chaque frame est :\n",
    "\n",
    "$$Y_{norm} = \\frac{Y_{poignet} - Y_{milieu\\_épaules}}{Distance(Épaule_{G}, Épaule_{D})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72bdf9-8374-403b-9b10-33a128fa3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectoires_normalisees(nom_fichier, nb_danseurs=1):\n",
    "    with open(nom_fichier, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    frames = []\n",
    "    # On prépare des listes pour stocker les poignets droits et gauches\n",
    "    traj_droit = [[] for _ in range(nb_danseurs)]\n",
    "    traj_gauche = [[] for _ in range(nb_danseurs)]\n",
    "    \n",
    "    for f_data in data:\n",
    "        frames.append(f_data['frame_id'])\n",
    "        # Tri des personnes de gauche à droite pour ne pas mélanger les courbes\n",
    "        insts = sorted(f_data['instances'], key=lambda x: np.mean([kp[0] for kp in x['keypoints']]))\n",
    "        \n",
    "        for i in range(nb_danseurs): #on tente d'éliminer les \"fantômes\" en saisissant manuellement le nombre supposé de danseurs dans nb_danseurs\n",
    "            if i < len(insts):\n",
    "                keypoints = np.array(insts[i]['keypoints'])\n",
    "                \n",
    "                # 1. Calcul du milieu des épaules (référence 0)\n",
    "                milieu_epaules = (keypoints[5][1] + keypoints[6][1]) / 2 #standard COCO human keypoints : 5=épaules gauche, 6=épaule droite\n",
    "                \n",
    "                # 2. Calcul de la largeur des épaules (unité de normalisation)\n",
    "                largeur_epaules = np.linalg.norm(np.array(keypoints[5][:2]) - np.array(keypoints[6][:2]))\n",
    "                largeur_epaules = largeur_epaules if largeur_epaules > 0 else 1\n",
    "                \n",
    "                # 3. Calcul de la position relative (Y_poignet - Y_épaules) / Largeur_Épaules\n",
    "                traj_droit[i].append((keypoints[10][1] - milieu_epaules) / largeur_epaules) #10=poignet droit\n",
    "                traj_gauche[i].append((keypoints[9][1] - milieu_epaules) / largeur_epaules) #9=poignet gauche\n",
    "            else:\n",
    "                traj_droit[i].append(None)\n",
    "                traj_gauche[i].append(None)\n",
    "                \n",
    "    return frames, traj_droit, traj_gauche\n",
    "\n",
    "\n",
    "frames, poignet_droit, poignet_gauche = trajectoires_normalisees('vis_results/predictions/original.json', nb_danseurs=1)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(frames, poignet_droit[0], color='blue', linewidth=2, label='Poignet droit')\n",
    "plt.plot(frames, poignet_gauche[0], color='green', linewidth=2, label='Poignet gauche')\n",
    "plt.axhline(0, color='red', linestyle='-', linewidth=0.8, label='Niveau Épaules')\n",
    "plt.title(\"Analyse des poignets gauche et droite\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Distance relative (Largeurs d'épaules)\")\n",
    "plt.gca().invert_yaxis() \n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "plt.grid(True, alpha=0.15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a94cfc-5ca0-498a-a977-f473ff5b3cf2",
   "metadata": {},
   "source": [
    "On remarque bien ici le mouvement des poignets par rapport aux épaules, c'est un mouvement répétitif et symétrique par rapport aux épaules ce qui va faciliter la prédiction. La ligne d'épaule ne bouge pas puisque c'est l'origine du repère.On observe ici uniquement le mouvement des poignets par rapport aux épaules et non par rapport à la caméra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e0342c-a63a-4ede-a228-c1632f147db4",
   "metadata": {},
   "source": [
    " ### IV. PRÉDICTION DES PROCHAINS MOUVEMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5345bb2-709e-4704-89a2-12f07498083d",
   "metadata": {},
   "source": [
    "Notre stratégie de modélisation se décomposera en deux temps :\n",
    "\n",
    "- Approche supervisée : implémentation d'un réseau récurrent type LSTM pour modéliser la dynamique temporelle de la danse. Ce modèle servira de référence pour la prédiction de trajectoires courtes.\n",
    "\n",
    "- Apprentissage par renforcement : utilisation de l'algorithme PPO (Proximal Policy Optimization) pour entraîner un agent virtuel à faire la danse de l'égyptien. L'environnement sera défini par la distance entre l'agent et la trajectoire normalisée du fichier original.json. Cette approche permettra de tester la robustesse du mouvement face à des perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48e516-4146-4e76-bb02-34f55529d130",
   "metadata": {},
   "source": [
    "#### IV.1) APPROCHE SUPERVISÉE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f16ee-075e-4727-9775-9ab7d6c4d78d",
   "metadata": {},
   "source": [
    "Le but ici est de prédire la frame t+1 en fonction des frames précédentes. Nous allons utiliser la MSE comme loss.\n",
    "\n",
    "Tout d'abbord, nous devons normaliser tous les points par rapport aux centre des épaules, jusqu'ici, nous avions étudié seulement les poignets mais pour la prédiction nous avons besoin de tous les keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad43adb-acaf-4297-bbf1-42c2675636b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stable_data(file_name, window_size=45):\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    all_frames_data = []\n",
    "    for f_data in data:\n",
    "        if len(f_data['instances']) > 0:\n",
    "            kp = np.array(f_data['instances'][0]['keypoints']) \n",
    "            # On centre sur les épaules\n",
    "            center = (kp[5] + kp[6]) / 2\n",
    "            # On normalise la taille\n",
    "            scale = np.linalg.norm(kp[5] - kp[6])\n",
    "            if scale == 0: scale = 1.0\n",
    "            norm_kp = (kp - center) / scale\n",
    "            all_frames_data.append(norm_kp.flatten())\n",
    "    \n",
    "    all_frames_data = np.array(all_frames_data)\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(all_frames_data) - window_size):\n",
    "        X.append(all_frames_data[i : i + window_size]) #toutes les frames sauf la dernière\n",
    "        y.append(all_frames_data[i + window_size]) # seulement la dernière frame\n",
    "        \n",
    "    return np.array(X), np.array(y), all_frames_data\n",
    "\n",
    "# On utilise window_size=30 pour un bon compromis mémoire/stabilité\n",
    "X_train, y_train, full_pos = prepare_stable_data('vis_results/predictions/original.json', window_size=20)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "print(f\"OK ! Données prêtes. X: {X_train.shape}, y: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd72cf-efe5-450a-a998-4b2700bbab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PosePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, num_layers=2):\n",
    "        super(PosePredictor, self).__init__()\n",
    "        \n",
    "        # On augmente hidden_dim à 512 pour plus de \"mémoire\" de la danse\n",
    "        # On ajoute dropout=0.2 pour éviter qu'il apprenne par cœur et qu'il bloque\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94634f43-f35d-41af-afb5-d2e0cc6b4669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = PosePredictor(input_dim=34, hidden_dim=512, num_layers=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 600 # On double les époques pour comprendre le rythme\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e666ea2-e91f-4a24-bd53-5ad3845f5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "input_seq = X_train_tensor[-1:]\n",
    "current_pos = full_pos[-1]\n",
    "\n",
    "generated_frames = []\n",
    "\n",
    "alpha = 1.75 # Plus ce chiffre est grand plus le modele va forcer ses mouvements\n",
    "\n",
    "nb_frame = 30\n",
    "with torch.no_grad():\n",
    "    for _ in range(nb_frame):\n",
    "        # On predit la pose \"prudente\"\n",
    "        pred_pose = model(input_seq).cpu().numpy().squeeze()\n",
    "        \n",
    "        # On calcule le mouvement prévu par rapport à la frame précédente\n",
    "        movement = pred_pose - input_seq[0, -1, :].cpu().numpy()\n",
    "        \n",
    "        # On rajoute ce mouvement multiplié par alpha à la dernière frame\n",
    "        boosted_pose = input_seq[0, -1, :].cpu().numpy() + (movement * alpha)\n",
    "        \n",
    "        # Pour que les bras ne s'allongent pas trop\n",
    "        boosted_pose = np.clip(boosted_pose, -3, 6)\n",
    "        \n",
    "        generated_frames.append(boosted_pose)\n",
    "        \n",
    "        # Mise à jour\n",
    "        new_frame = torch.from_numpy(boosted_pose).float().to(device).view(1, 1, 34)\n",
    "        input_seq = torch.cat((input_seq[:, 1:, :], new_frame), dim=1)\n",
    "\n",
    "generated_frames = np.array(generated_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a91d78-0a29-4920-a8ec-a3cf58aef8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def final_dance_show(full_pos, generated_frames):\n",
    "    # On prend les 30 dernières frames du vrai danseur\n",
    "    real_part = full_pos[-nb_frame:]\n",
    "    \n",
    "    # On applique le modèle\n",
    "    all_dance = np.concatenate([real_part, generated_frames])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 7), facecolor='black')\n",
    "    \n",
    "    def update(i):\n",
    "        ax.clear()\n",
    "        ax.set_facecolor('black')\n",
    "        ax.set_xlim(-5, 5)\n",
    "        ax.set_ylim(10, -5) \n",
    "        ax.axis('off')\n",
    "        \n",
    "        kp = all_dance[i].reshape(17, 2)\n",
    "        color = '#00FF00' if i < nb_frame else '#00FFFF' \n",
    "        \n",
    "        links = [(5,6),(5,7),(7,9),(6,8),(8,10),(5,11),(6,12),(11,12),(11,13),(13,15),(12,14),(14,16)]\n",
    "        for s, e in links:\n",
    "            ax.plot([kp[s,0], kp[e,0]], [kp[s,1], kp[e,1]], color=color, lw=3)\n",
    "        ax.scatter(kp[:,0], kp[:,1], c='white', s=20)\n",
    "        ax.set_title(\"RÉEL (VERT) puis IA GÉNÉRÉE (CYAN)\", color='white')\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(all_dance), interval=33)\n",
    "    plt.close()\n",
    "    return ani\n",
    "\n",
    "HTML(final_dance_show(full_pos, generated_frames).to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9651add-f73c-406e-ab21-7eb43fdc0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_r2 = r2_score(full_pos[-nb_frame:].flatten(), generated_frames.flatten())\n",
    "print(score_r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (OpenMMLab)",
   "language": "python",
   "name": "openmmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
